{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Komentar Twitter Pengguna Media Sosial Untuk Pemilu 2014\n",
    "\n",
    "By Jonathan Stanley\n",
    "\n",
    "Contact:\n",
    "\n",
    "jonathanstanleyofficial@gmail.com / 082112426652\n",
    "\n",
    "\n",
    "\n",
    "Proyek ini memanfaatkan model BERT untuk melakukan sentiment analysis terhadap tweet para pengguna sosial media yang berkaitan dengan Pemilu Presiden 2014. Model yang dibuat diharapkan dapat mengklasifikasikan isi tweet yang positif dan negatif perihal isu tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading and Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3XpRjzR4yrTB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Reading\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"Capres2014-1.1.csv\", usecols=[\"Isi_Tweet\", \"Sentimen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iTg9R7gExRJt",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Isi_Tweet</th>\n",
       "      <th>Sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biusnya habis ! RT\"@eddies_song: Dahlan Iskan ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Presiden Prabowo ,Presiden Terakhir Indonesia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@republikaonline masa capres prabowo bergitu b...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kalo kata bapak capres ARB, kita harus \"berani...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @DhafaRizky_: Najis,org gila doang yg dukun...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Isi_Tweet  Sentimen\n",
       "0  Biusnya habis ! RT\"@eddies_song: Dahlan Iskan ...        -1\n",
       "1      Presiden Prabowo ,Presiden Terakhir Indonesia         1\n",
       "2  @republikaonline masa capres prabowo bergitu b...        -1\n",
       "3  Kalo kata bapak capres ARB, kita harus \"berani...         1\n",
       "4  RT @DhafaRizky_: Najis,org gila doang yg dukun...        -1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXPLORE the data\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset yang digunakan memiliki dua komponen fitur, Fitur pertama adalah isi tweet dengan bentuk text dan fitur kedua adalah Sentimen yang telah diberi label -1 untuk tweet negatif dan 1 untuk tweet positif. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dOb9J-hkxRJz",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentimen\n",
       " 1    1117\n",
       "-1     768\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Mengecek Imbalanced Data\n",
    "dataset['Sentimen'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "n468ybk-xRJ4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Mengganti {-1,1} menjadi {0,1}\n",
    "dataset['Sentimen'] = dataset['Sentimen'].replace(-1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "nltk.download('stopwords')\n",
    "#Setting indonesian stopwords\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "#Stemming indonesian words\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "my1D6A1fxRJ8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1885, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Pra Pengolahan - Cleaning\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(tweet):\n",
    "    \n",
    "    # Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    # remove unicode characters\n",
    "    tweet = tweet.encode('ascii', 'ignore').decode()\n",
    "    # Clean www.* or https?://*\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    # Clean @username\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    #Remove common Indonesian stop words and stemming Indonesian words\n",
    "    tweet_tokens = tweet.split()\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stop_words]\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "    tweet = ' '.join(stemmed_words)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "dataset[\"Isi_Tweet\"] = dataset['Isi_Tweet'].map(lambda x: clean_text(x))\n",
    "dataset = dataset[dataset['Isi_Tweet'].apply(lambda x: len(x.split()) >=1)]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PObZq4wExRKF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Pra Pengolahan - Splitting (80% training : 20% testing)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    dataset['Isi_Tweet'], dataset['Sentimen'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tk62LS7PGRl0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/85/f6/c5065913119c41ecad148c34e3a861f719e16b89a522287213698da911fc/transformers-4.37.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "     ---------------------------------------- 0.0/129.4 kB ? eta -:--:--\n",
      "     --------------------------------- ---- 112.6/129.4 kB 2.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 129.4/129.4 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting filelock (from transformers)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/28/03/7d3c7153113ec59cfb31e3b8ee773f5f420a0dd7d26d40442542b96675c3/huggingface_hub-0.20.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/c1/02/40725eebedea8175918bd59ab80b2174d6ef3b3ef9ac8ec996e84c38d3ca/tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/54/ce/09e508c682c612e7ff7b5cf1249a963d10db58f16d77007177f7770c661b/safetensors-0.4.2-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.2-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/ad/30/2281c062222dc39328843bd1ddd30ff3005ef8e30b2fd09c4d2792766061/fsspec-2024.2.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "   ---------------------------------------- 0.0/8.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/8.4 MB 3.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.2/8.4 MB 2.4 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/8.4 MB 2.2 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/8.4 MB 3.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/8.4 MB 4.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.5/8.4 MB 5.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.9/8.4 MB 5.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.3/8.4 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.7/8.4 MB 6.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.1/8.4 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.5/8.4 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.9/8.4 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.3/8.4 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.7/8.4 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.1/8.4 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.1/8.4 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.9/8.4 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.3/8.4 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.8/8.4 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.2/8.4 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.6/8.4 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.0/8.4 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.4/8.4 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.4/8.4 MB 7.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 330.1/330.1 kB 6.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp311-none-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 269.6/269.6 kB 8.4 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.4/2.2 MB 8.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.2 MB 8.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.7/2.2 MB 9.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 6.1 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "   ---------------------------------------- 0.0/170.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 170.9/170.9 kB 10.7 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.1 fsspec-2024.2.0 huggingface-hub-0.20.3 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.37.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.37.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Rb2oAZPeAtN5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at indobenchmark/indobert-base-p2 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at indobenchmark/indobert-base-p2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Pretrained model Indobert\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "def tokenisasi(teks):\n",
    "    encode_dict = bert_tokenizer(teks,\n",
    "                                   add_special_tokens = True,\n",
    "                                   max_length = 128, \n",
    "                                   padding = 'max_length',\n",
    "                                   truncation = True,\n",
    "                                   return_attention_mask = True,\n",
    "                                   return_tensors = 'tf',)\n",
    "\n",
    "    tokenID = encode_dict['input_ids']\n",
    "    attention_mask = encode_dict['attention_mask']\n",
    "    return tokenID, attention_mask\n",
    "\n",
    "def create_input(data):\n",
    "    tokenID, input_mask = [], []\n",
    "    for teks in data:\n",
    "        token, mask = tokenisasi(teks)\n",
    "        tokenID.append(token)\n",
    "        input_mask.append(mask)\n",
    "    \n",
    "    return [np.asarray(tokenID, dtype=np.int32).reshape(-1, 128), \n",
    "            np.asarray(input_mask, dtype=np.int32).reshape(-1, 128)]\n",
    "\n",
    "bert_model = TFAutoModel.from_pretrained(\"indobenchmark/indobert-base-p2\", trainable=False)\n",
    "\n",
    "def bert(hp):\n",
    "    \n",
    "    #Input layer\n",
    "    input_token = keras.layers.Input(shape=(128,), dtype=np.int32,\n",
    "                                        name=\"input_token\")\n",
    "    input_mask = keras.layers.Input(shape=(128,), dtype=np.int32,\n",
    "                                   name=\"input_mask\")\n",
    "\n",
    "    #Embedding\n",
    "    bert_embedding = bert_model([input_token, input_mask])[0]\n",
    "    \n",
    "    \n",
    "    # Attention mechanism\n",
    "    num_heads = hp.Int('num_heads', min_value=2, max_value=8, step=2)\n",
    "    attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=128)(bert_embedding, bert_embedding, bert_embedding)\n",
    "    add_attention = keras.layers.Add()([bert_embedding, attention])\n",
    "    layer_norm1 = keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "    \n",
    "    #Dropout Layer\n",
    "    dropout_rate = 0.2\n",
    "    dropout_layer = keras.layers.Dropout(dropout_rate)(layer_norm1)\n",
    "\n",
    "    #Output layer\n",
    "    output = keras.layers.Dense(1, activation='sigmoid',\n",
    "                                kernel_regularizer=keras.regularizers.l2(hp.Choice('kernel_dense', values = [0.01, 0.001])))(dropout_layer)\n",
    "    \n",
    "    \n",
    "    #Adjust Learning Rates\n",
    "    learning_rate = 1e-3\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        learning_rate,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.95,\n",
    "        staircase=True\n",
    "    )\n",
    "    \n",
    "    #Model Compiler\n",
    "    model = keras.models.Model(inputs=[input_token, input_mask], outputs=output)\n",
    "\n",
    "    model.compile(optimizer = keras.optimizers.Adam(lr_schedule),\n",
    "                  loss ='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "   \n",
    "    return model\n",
    "\n",
    "class ClearTrainingOutput(keras.callbacks.Callback):\n",
    "    def on_train_end(*args, **kwargs):\n",
    "        IPython.display.clear_output(wait = True)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wBICCOipCWxr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "  Obtaining dependency information for keras-tuner from https://files.pythonhosted.org/packages/2b/39/21f819fcda657c37519cf817ca1cd03a8a025262aad360876d2a971d38b3/keras_tuner-1.4.6-py3-none-any.whl.metadata\n",
      "  Downloading keras_tuner-1.4.6-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras-tuner) (2.12.0rc1)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras-tuner) (23.0)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras-tuner) (2.28.2)\n",
      "Collecting kt-legacy (from keras-tuner)\n",
      "  Obtaining dependency information for kt-legacy from https://files.pythonhosted.org/packages/16/53/aca9f36da2516db008017db85a1f3cafaee0efc5fc7a25d94c909651792f/kt_legacy-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->keras-tuner) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->keras-tuner) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->keras-tuner) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->keras-tuner) (2022.12.7)\n",
      "Downloading keras_tuner-1.4.6-py3-none-any.whl (128 kB)\n",
      "   ---------------------------------------- 0.0/128.9 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 30.7/128.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 92.2/128.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 128.9/128.9 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.6 kt-legacy-1.0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [01h 25m 47s]\n",
      "val_accuracy: 0.8386936187744141\n",
      "\n",
      "Best val_accuracy So Far: 0.8514589071273804\n",
      "Total elapsed time: 13h 55m 59s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner.tuners import BayesianOptimization\n",
    "\n",
    "bert_train_data = create_input(train_data)\n",
    "bert_test_data = create_input(test_data)\n",
    "\n",
    "tuner = BayesianOptimization(bert,\n",
    "                             objective = 'val_accuracy', \n",
    "                             max_trials = 10,\n",
    "                             directory = '/content/Hasil',\n",
    "                             project_name = 'Sentiment-BERT',\n",
    "                             overwrite = True)\n",
    "\n",
    "tuner.search(bert_train_data, train_labels,\n",
    "             batch_size=256, epochs = 50,\n",
    "             validation_data=(bert_test_data, test_labels),\n",
    "             callbacks=[early_stop, ClearTrainingOutput()])\n",
    "\n",
    "# Mendapatkan model terbaik\n",
    "model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "zD4U3ONwxRKy",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 59s 5s/step - loss: 0.5048 - accuracy: 0.8515\n",
      "Test accuracy: 0.8514589071273804\n"
     ]
    }
   ],
   "source": [
    "## Evaluasi Model\n",
    "\n",
    "test_loss, test_acc = model.evaluate(bert_test_data, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Evaluation\n",
    "\n",
    "Hasil prediksi model BERT menunjukkan performa yang cukup baik untuk menjalankan sentiment analysis pada project ini. Akurasi prediksi yang diperoleh sebesar 0.8515 sehingga kurang lebih 85% dari prediksi telah sesuai dengan data sentiment sebenarnya pada testing set yang digunakan untuk evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EH3To9fpxRK2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Penyimpanan dan Memuat Kembali Model\n",
    "model.save('Data/model_mlp_sentiment.h5')\n",
    "\n",
    "model = keras.models.load_model('Data/model_mlp_sentiment.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
